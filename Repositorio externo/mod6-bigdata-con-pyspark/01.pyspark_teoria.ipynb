{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark\n",
    "\n",
    "**PySpark** es la interfaz de programación de **Python** para el framework de procesamiento distribuido **Apache Spark**.\n",
    "\n",
    "**Spark** es un motor de procesamiento de datos distribuido y de alto rendimiento que se utiliza para procesar grandes volúmenes de datos de manera escalable y eficiente en clústeres de computadoras.\n",
    "\n",
    "**PySpark** se utiliza comúnmente para tareas de procesamiento de datos, aprendizaje automático, análisis de datos en tiempo real, y para la construcción de aplicaciones de procesamiento de grandes volúmenes de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark_teoria\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numero de nucleos\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer un archivo con PySpark\n",
    "titanic = spark.read.csv(path        = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/titanic.txt\",\n",
    "                         inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.show(5, truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.schema[\"Ticket\"].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.select(\"age\", \"fare\").summary(\"count\", \"min\", \"max\", \"mean\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Espeficicar dtypes de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark reconoce todos como strings\n",
    "\n",
    "people = spark.read.json(path = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/people.json\")\n",
    "\n",
    "print(people.printSchema())\n",
    "\n",
    "people.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos el dtype de \"timestamp\" a DateType()\n",
    "\n",
    "data_schema = list((StructField(\"name\"      , StringType(), True),\n",
    "                    StructField(\"email\"     , StringType(), True),\n",
    "                    StructField(\"city\"      , StringType(), True),\n",
    "                    StructField(\"mac\"       , StringType(), True),\n",
    "                    StructField(\"timestamp\" ,   DateType(), True),\n",
    "                    StructField(\"creditcard\", StringType(), True)))\n",
    "\n",
    "final_struc = StructType(fields = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el archivo otra vez pero especificando el schema\n",
    "\n",
    "people = spark.read.json(path   = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/people.json\",\n",
    "                         schema = final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar y Filtrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa = spark.read.csv(path        = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/fifa19.csv\",\n",
    "                      inferSchema = True, header = True)\n",
    "\n",
    "fifa.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fifa.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para seleccionar columnas usamos .select y pasamos una lista con las columnas (los corchetes son opcionales)\n",
    "\n",
    "fifa.select([\"Nationality\", \"Name\", \"Age\", \"Photo\"]).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OrderBy, por defecto ascending = True\n",
    "\n",
    "fifa.select([\"Name\", \"Age\"])\\\n",
    "    .orderBy(fifa[\"Age\"]).show(5)\n",
    "\n",
    "#fifa.select([\"Name\", \"Age\"])\\\n",
    "#    .orderBy(fifa[\"Age\"].asc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .desc()\n",
    "\n",
    "fifa.select([\"Name\", \"Age\"])\\\n",
    "    .orderBy(fifa[\"Age\"].desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para filtrar por palabras podemos usar .where en conjunto con .like\n",
    "\n",
    "fifa.select([\"Name\", \"Club\"])\\\n",
    "    .where(fifa.Club.like(\"%Barcelona%\")).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos utilizar .substr() para hacer \"slicing\" a una cadena de caracteres\n",
    "\n",
    "fifa.select(\"Photo\", fifa.Photo.substr(-4, 4)).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .isin similar a Pandas\n",
    "\n",
    "fifa[fifa.Club.isin(\"FC Barcelona\", \"Juventus\")].limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .where(), .startswith() y .endswith()\n",
    "# Nota: los .where van uno detrás de otro.\n",
    "\n",
    "# fifa.select(\"Name\", \"Club\").where(fifa.Name.startswith(\"L\")).where(fifa.Name.endswith(\"i\")).show(5)\n",
    "\n",
    "fifa.select(\"Name\", \"Club\")                \\\n",
    "    .where(fifa.Name.startswith(\"L\"))      \\\n",
    "    .where(fifa.Name.endswith(\"i\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape[0]\n",
    "\n",
    "fifa.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .limit() para seleccionar el número de filas\n",
    "\n",
    "df3 = fifa.limit(100)\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con las primeras 5 columnas\n",
    "\n",
    "col_list = fifa.columns[:5]\n",
    "df3 = fifa.select(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuevo df\n",
    "df3.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .filter(condicion)\n",
    "\n",
    "fifa.filter(\"Overall > 50\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos usar .filter en conjunto con .select\n",
    "\n",
    "fifa.filter(\"Overall > 50\").select([\"Name\", \"Age\"]).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El orden no afecta el output .select .filter\n",
    "\n",
    "fifa.select([\"Name\", \"Age\"]).filter(\"Overall > 50\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varias condiciones AND & OR\n",
    "\n",
    "fifa.select([\"Name\", \"Age\", \"Club\"]).filter(\"Overall > 50 AND Age < 30 AND Club = 'FC Barcelona'\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa.select([\"Name\", \"Age\", \"Club\"]).filter(\"Club = 'Juventus' OR Club = 'FC Barcelona'\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .collect() \"transforma\" el output a list\n",
    "\n",
    "result = fifa.filter(\"Overall > 50\")                           \\\n",
    "             .select([\"Nationality\", \"Name\", \"Age\", \"Overall\"])\\\n",
    "             .orderBy(fifa[\"Overall\"].desc()).collect()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "print(\"Mejor jugador Overall>50\", result[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifa\n",
    "print(\"Mejor jugador Overall>50\", fifa[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "print(\"Peor jugador Overall<50\", result[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulacion de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# concat_ws()\n",
    "\n",
    "concat = fifa.select(fifa.Name,\n",
    "                     fifa.Nationality,\n",
    "                     concat_ws(\" \", fifa.Name, fifa.Nationality).alias(\"Nombre/Nacionalidad\"))\n",
    "\n",
    "concat.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat.rdd.id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevo df\n",
    "\n",
    "videos = spark.read.csv(path = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/youtubevideos.csv\",\n",
    "                        header = True, inferSchema = True)\n",
    "\n",
    "videos.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos reasignar las columnas usando .withColumn en conjunto con .cast, to_date o to_timestamp\n",
    "\n",
    "df = videos.withColumn(\"views\"        , videos[\"views\"].cast(IntegerType()))                        \\\n",
    "           .withColumn(\"likes\"        , videos[\"likes\"].cast(IntegerType()))                        \\\n",
    "           .withColumn(\"dislikes\"     , videos[\"dislikes\"].cast(IntegerType()))                     \\\n",
    "           .withColumn(\"category_id\"  , videos[\"category_id\"].cast(IntegerType()))                  \\\n",
    "           .withColumn(\"trending_date\", to_date(videos.trending_date, \"yy.dd.mm\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .withColumn() también nos permite crear columnas a partir de otras\n",
    "\n",
    "df = df.withColumn(\"publish_time_2\", regexp_replace(df.publish_time, \"T\", \" \"))\n",
    "df = df.withColumn(\"publish_time_2\", regexp_replace(df.publish_time_2, \"Z\", \"\"))\n",
    "\n",
    "df.select(\"publish_time\", \"publish_time_2\").show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower()\n",
    "df.select(\"title\", lower(df.title)).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when(), puede crear columnas a partir de otras si se cumple cierta condición\n",
    "\n",
    "df.select(\"likes\",\n",
    "          \"dislikes\",\n",
    "          (when(df.likes > df.dislikes, \"Good\").when(df.likes < df.dislikes, \"Bad\").when(df.likes == df.dislikes, \"Equal\")\\\n",
    "          .otherwise(\"Undetermined\")).alias(\"Favorability\")).show(5)\n",
    "\n",
    "# otherwise() se usa cuando no se resuelve la condicion, y esto puede suceder, por ejemplo, cuando hay NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expr\n",
    "\n",
    "# con expr podemos escribir en sintaxis SQL como queremos la nueva columna\n",
    "\n",
    "df.select(\"likes\",\n",
    "          \"dislikes\",\n",
    "          expr(\"CASE WHEN likes > dislikes THEN 'Good' \\\n",
    "                     WHEN dislikes > likes THEN 'Bad'  \\\n",
    "                     ELSE 'Undetermined' END           \\\n",
    "                AS Favorability\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year() y month()\n",
    "# Esto funciona porque la columna esta en formato DateType()\n",
    "\n",
    "df.select(\"trending_date\",\n",
    "          year(\"trending_date\").alias(\"year\"),\n",
    "          month(\"trending_date\").alias(\"month\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datediff()\n",
    "# Esto funciona porque las columnas estan en formato DateType()\n",
    "\n",
    "df.select(\"trending_date\",\n",
    "          \"publish_time_2\",\n",
    "          datediff(df.publish_time_2, df.trending_date)).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split()\n",
    "array = df.select(\"title\",\n",
    "                  split(df.title, \" \").alias(\"split\"))\n",
    "\n",
    "array.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array_contains parecido a \"in\" en python\n",
    "\n",
    "array.select(\"split\",\n",
    "             array_contains(array.split, \"(HBO)\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array_distinct parecido a .unique() en Pandas\n",
    "\n",
    "array.select(\"title\", array_distinct(array.split)).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array_remove eliminar un elemento de un array \n",
    "\n",
    "array.select(\"title\", array_remove(array.split, \"Presidency:\")).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos usar funciones para crear nuevas columnas\n",
    "\n",
    "from pyspark.sql.functions import udf          # user define functions\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El retorno de lambda \n",
    "\n",
    "def square(x):\n",
    "    return int(x**2)\n",
    "\n",
    "square_udf = udf(f          = lambda x : square(x),\n",
    "                 returnType = IntegerType())\n",
    "\n",
    "df.select(\"dislikes\",\n",
    "          square_udf(\"dislikes\").alias(\"dislikes**2\")).where(col(\"dislikes\").isNotNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Si ejecutamos sin usar .isNotNull() nos dará error porque hay NaN's\n",
    "# df.select(\"dislikes\", square_udf(\"dislikes\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# igual que la funcion .groupBy() y .agg() de pandas\n",
    "\n",
    "fifa.groupBy(\"Club\", \"Nationality\").agg({\"ID\" : \"count\"}).show(1_000, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_fifa = pd.read_csv(filepath_or_buffer = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/fifa19.csv\")\n",
    "\n",
    "df_fifa.groupby([\"Club\", \"Nationality\"]).agg({\"ID\" : \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con esta notación podemos agregar .alias a las columnas\n",
    "\n",
    "fifa.groupBy(\"Club\").agg(min(fifa.Age).alias(\"Min Age\"),\n",
    "                         max(fifa.Age).alias(\"Max Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con .summary() podemos obtener un resultado similar\n",
    "\n",
    "videos.select(\"views\", \"likes\", \"dislikes\")                                      \\\n",
    "      .summary(\"count\", \"min\", \"25%\", \"75%\", \"max\", \"stddev\").limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic1 = spark.read.csv(path = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/titanic 1.csv\",\n",
    "                          inferSchema = True, header = True)\n",
    "\n",
    "titanic2 = spark.read.csv(path = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/titanic 2.csv\",\n",
    "                          inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic1.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .union funciona como pd.concat, solo funciona para axis = 0\n",
    "# Los dfs deben tener la misma cantidad de columnas para funcionar\n",
    "# Agrega las filas\n",
    "\n",
    "titanic = titanic1.union(titanic1)\n",
    "\n",
    "print(titanic1.count())\n",
    "print(titanic.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Joins\n",
    "titanic = titanic1.join(other = titanic2, on = [\"PassengerId\"], how = \"inner\")\n",
    "\n",
    "titanic.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos con isNull()\n",
    "\n",
    "titanic.select([\"Name\", \"PassengerId\", \"Age\"]).filter(titanic.Age.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con esta funcion podemos contar cuantas filas tienen NaN's\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = list()\n",
    "    numRows = df.count()\n",
    "    \n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        \n",
    "        if (nullRows > 0):\n",
    "            temp = k, nullRows, (nullRows / numRows)*100\n",
    "            null_columns_counts.append(temp)\n",
    "            \n",
    "    return null_columns_counts\n",
    "\n",
    "null_columns_calc_list = null_value_calc(titanic)\n",
    "\n",
    "null_columns_calc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(data = null_columns_calc_list,\n",
    "                      schema = [\"Name\", \"Count\", \"Percent\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.na.drop() = df.dropna()\n",
    "\n",
    "titanic.na.drop().limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .na.drop() sin parametros\n",
    "\n",
    "og_len = titanic.count()\n",
    "drop_len = titanic.na.drop().count()\n",
    "\n",
    "print(\"Filas eliminadas\", og_len - drop_len)\n",
    "print(\"Porcentaje de filas eliminadas\", (og_len - drop_len)/og_len*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .na.drop() con threshold = 8\n",
    "\n",
    "og_len = titanic.count()\n",
    "drop_len = titanic.na.drop(thresh = 8).count()\n",
    "\n",
    "print(\"Filas eliminadas\", og_len - drop_len)\n",
    "print(\"Porcentaje de filas eliminadas\", (og_len - drop_len)/og_len*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .na.drop() con threshold = 6\n",
    "\n",
    "og_len = titanic.count()\n",
    "drop_len = titanic.na.drop(thresh = 6).count()\n",
    "print(\"Filas eliminadas\", og_len - drop_len)\n",
    "\n",
    "print(\"Porcentaje de filas eliminadas\", (og_len - drop_len)/og_len*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .na.drop() podemos elegir por cual columna eliminar las filas\n",
    "\n",
    "og_len = titanic.count()\n",
    "drop_len = titanic.na.drop(subset = [\"Age\"]).count()\n",
    "\n",
    "print(\"Filas eliminadas\", og_len - drop_len)\n",
    "print(\"Porcentaje de filas eliminadas\", (og_len - drop_len)/og_len*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .na.drop() con how = \"all\" (toda la fila debe tener NaN's)\n",
    "\n",
    "og_len = titanic.count()\n",
    "drop_len = titanic.na.drop(how = \"all\").count()\n",
    "\n",
    "print(\"Filas eliminadas\", og_len - drop_len)\n",
    "print(\"Porcentaje de filas eliminadas\", (og_len - drop_len)/og_len*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na.fill(value), \"value\" debe coincidir con el dtype de la columna\n",
    "# Si esto no se cumple, na.fill() no hará nada\n",
    "\n",
    "titanic.na.fill(value = 9999).limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fila 6\n",
    "titanic.na.fill(value = \"NO AGE\").limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos hacer fill a una columna especifica\n",
    "\n",
    "titanic.na.fill(value = 9999, subset = [\"Age\"]).limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En una linea\n",
    "\n",
    "titanic.filter(titanic.Age.isNull()).na.fill(value = 9999, subset = [\"Age\"]).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambia los NaN's por el promedio de la columna\n",
    "\n",
    "def fill_with_mean(df, include = set()):\n",
    "    stats = df.agg(*(avg(c).alias(c) for c in df.columns if c in include))\n",
    "    \n",
    "    return df.na.fill(value = stats.first().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = fill_with_mean(titanic, [\"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fila 6\n",
    "updated_df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
