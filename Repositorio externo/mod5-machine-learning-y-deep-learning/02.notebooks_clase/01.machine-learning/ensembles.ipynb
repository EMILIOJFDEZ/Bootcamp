{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de conjunto\n",
    "\n",
    "sklearn.ensemble\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble\n",
    "\n",
    "* Bagging\n",
    "    * BaggingClassifier\n",
    "    * BaggingRegressor\n",
    "    * RandomForestClassifier\n",
    "    * RandomForestRegressor\n",
    "    * ExtraTreesClassifier\n",
    "    * ExtraTreesRegressor\n",
    "\n",
    "* Voting\n",
    "\t* VotingClassifier\n",
    "    * VotingRegressor\n",
    "\n",
    "* Boosting\n",
    "    * AdaBoostClassifier\n",
    "    * AdaBoostRegressor\n",
    "    * GradientBoostingClassifier\n",
    "    * GradientBoostingRegressor\n",
    "\n",
    "* Stacking\n",
    "\t* StackingClassifier\n",
    "\t* StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "df = sns.load_dataset('mpg').dropna()\n",
    "\n",
    "features = ['weight', 'cylinders', 'displacement', 'horsepower', 'acceleration', 'model_year']\n",
    "X = df[features]\n",
    "y = df['mpg']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bagging (Bootstrap Aggregating)\n",
    "\n",
    "- **Idea principal**: \n",
    "  - Entrenar varios modelos (generalmente iguales) en muestras *bootstrap* (subconjuntos del dataset con reemplazo) y luego combinar sus predicciones (votación mayoritaria para clasificación o promedio para regresión).  Construye múltiples modelos a partir de diferentes submuestras del conjunto de datos de entrenamiento.\n",
    "  - Los modelos base se entrenan de forma **independiente** y en **paralelo** sobre estos subconjuntos.\n",
    "  - El término \"reemplazo\" en bagging se refiere al método de muestreo con reemplazo. Esto significa que al extraer muestras del conjunto de datos original para formar cada subconjunto (o \"bootstrap sample\"), cada instancia se selecciona de forma aleatoria y, después de ser elegida, se vuelve a colocar en el conjunto, permitiendo que pueda ser seleccionada nuevamente.\n",
    "  - Este mecanismo de muestreo con reemplazo contribuye a la diversidad de los modelos base, lo que es clave para la eficacia del bagging en la reducción de la varianza.\n",
    "- **Objetivo**: Reducir la varianza y mejorar la estabilidad.  \n",
    "- **Ejemplos en Scikit-Learn**:\n",
    "  - `BaggingClassifier`, `BaggingRegressor`: Métodos genéricos de bagging para cualquier estimador base.\n",
    "  - `RandomForestClassifier`, `RandomForestRegressor`: Casos populares de bagging con árboles y muestreo de características.\n",
    "  - `ExtraTreesClassifier`, `ExtraTreesRegressor`: Similar a RandomForest pero con mayor aleatoriedad al dividir nodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingRegressor: R2 = 0.842, MAE = 2.049, RMSE = 2.838, MAPE = 0.091\n",
      "RandomForestRegressor: R2 = 0.885, MAE = 1.761, RMSE = 2.423, MAPE = 0.080\n",
      "ExtraTreesRegressor: R2 = 0.896, MAE = 1.693, RMSE = 2.305, MAPE = 0.076\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "models = {\n",
    "    'BaggingRegressor': BaggingRegressor(random_state=42), # por defecto usa DecisionTreeRegressor si no especificamos nada\n",
    "    'RandomForestRegressor': RandomForestRegressor(random_state=42),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{name}: R2 = {r2_score(y_test, y_pred):.3f}, MAE = {mean_absolute_error(y_test, y_pred):.3f}, RMSE = {root_mean_squared_error(y_test, y_pred):.3f}, MAPE = {mean_absolute_percentage_error(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Voting\n",
    "\n",
    "- **Idea principal**: es un complemento de bagging. Combina diferentes estimadores (pueden ser modelos distintos) entrenados en el **mismo** conjunto de datos.  \n",
    "- **Cómo se combinan**:  \n",
    "  - Clasificación: votación por mayoría (hard) o promedio de probabilidades (soft).  \n",
    "  - Regresión: se hace un promedio (o combinación) de las salidas.  \n",
    "- **Ejemplos en Scikit-Learn**:\n",
    "  - `VotingClassifier`  \n",
    "  - `VotingRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingRegressor: R2 = 0.841, MAE = 2.114, RMSE = 2.846, MAPE = 0.095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "model_1 = LinearRegression()\n",
    "model_2 = KNeighborsRegressor()\n",
    "model_3 = SVR()\n",
    "model_4 = DecisionTreeRegressor()\n",
    "\n",
    "model = VotingRegressor([\n",
    "    ('linear_regression', model_1),\n",
    "    ('knn', model_2),\n",
    "    # ('svr', model_3),\n",
    "    ('cart', model_4),\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"VotingRegressor: R2 = {r2_score(y_test, y_pred):.3f}, MAE = {mean_absolute_error(y_test, y_pred):.3f}, RMSE = {root_mean_squared_error(y_test, y_pred):.3f}, MAPE = {mean_absolute_percentage_error(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingRegressor: R2 = 0.897, MAE = 1.664, RMSE = 2.293, MAPE = 0.075\n"
     ]
    }
   ],
   "source": [
    "model_1 = RandomForestRegressor()\n",
    "model_2 = ExtraTreesRegressor()\n",
    "\n",
    "model = VotingRegressor([\n",
    "    ('rf', model_1),\n",
    "    ('et', model_2),\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"VotingRegressor: R2 = {r2_score(y_test, y_pred):.3f}, MAE = {mean_absolute_error(y_test, y_pred):.3f}, RMSE = {root_mean_squared_error(y_test, y_pred):.3f}, MAPE = {mean_absolute_percentage_error(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting\n",
    "\n",
    "- **Idea principal**: Entrenar de forma secuencial múltiples “weak learners”; cada nuevo modelo se centra en corregir los errores de los modelos anteriores.  \n",
    "- **Cómo funciona**: Ajustar iterativamente modelos base (por ejemplo, árboles pequeños) dando más peso a los errores o los residuos en cada paso. \n",
    "  - Durante el proceso, las observaciones que fueron mal clasificadas adquieren mayor peso, haciendo que los modelos sucesivos se concentren en las áreas donde el rendimiento es deficiente.\n",
    "  - Al enfocarse en los errores, boosting busca construir un modelo final más fuerte combinando varios modelos base débiles, lo que permite reducir el sesgo.\n",
    "- **Ejemplos en Scikit-Learn**:\n",
    "  - `AdaBoostClassifier`, `AdaBoostRegressor`: Ajusta pesos de los ejemplos para corregir errores.  \n",
    "  - `GradientBoostingClassifier`, `GradientBoostingRegressor`: Ajuste secuencial basado en el gradiente de la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostRegressor: R2 = 0.818, MAE = 2.240, RMSE = 3.048, MAPE = 0.103\n",
      "GradientBoostingRegressor: R2 = 0.869, MAE = 1.831, RMSE = 2.582, MAPE = 0.082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "models = {\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(random_state=42), # 50 estimators por defecto, se puede cambiar el modelo base\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor() # 100 estimators por defecto, usa DecisionTreeRegressor\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{name}: R2 = {r2_score(y_test, y_pred):.3f}, MAE = {mean_absolute_error(y_test, y_pred):.3f}, RMSE = {root_mean_squared_error(y_test, y_pred):.3f}, MAPE = {mean_absolute_percentage_error(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stacking (Stacked generalization)\n",
    "\n",
    "- **Idea principal**: Entrenar varios modelos base y luego usar **sus predicciones** como entradas para un modelo “meta” que aprende la mejor forma de combinarlas.  \n",
    "- **Cómo funciona**: \n",
    "  1. Entrenar los modelos base.  \n",
    "  2. Usar sus predicciones para alimentar el metamodelo final. (final_estimator)\n",
    "  3. El metamodelo combina los outputs de los anteriores.  \n",
    "- **Ejemplos en Scikit-Learn**:\n",
    "  - `StackingClassifier`  \n",
    "  - `StackingRegressor`\n",
    "\n",
    "No hay correcciones como en Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingRegressor: R2 = 0.848, MAE = 2.063, RMSE = 2.781, MAPE = 0.091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "model = StackingRegressor([ # lista de modelos base\n",
    "        ('linear_regression', LinearRegression()),\n",
    "        ('knn', KNeighborsRegressor()),\n",
    "        ('svr', SVR()),\n",
    "        ('cart', DecisionTreeRegressor()),\n",
    "    ], \n",
    "    final_estimator=RandomForestRegressor(random_state=42) # meta estimator se entrena usando como entrada las predicciones de los modelos base\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"VotingRegressor: R2 = {r2_score(y_test, y_pred):.3f}, MAE = {mean_absolute_error(y_test, y_pred):.3f}, RMSE = {root_mean_squared_error(y_test, y_pred):.3f}, MAPE = {mean_absolute_percentage_error(y_test, y_pred):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
