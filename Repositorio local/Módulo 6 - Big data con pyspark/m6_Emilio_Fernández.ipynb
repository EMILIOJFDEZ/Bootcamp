{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo 6: Big data con pyspark - Ejercicio de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "from pyspark.ml.feature import StringIndexer, Imputer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de datos de diamonds desde CSV con schema:\n",
    "\n",
    "- Primero, se descarga el dataset desde un enlace en línea y se guarda localmente.\n",
    "\n",
    "- Luego, se define un esquema para las columnas de datos (especificando los tipos de datos como DoubleType o StringType)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset desde la URL y guardarlo en CSV localmente\n",
    "dataset_url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/diamonds.csv'\n",
    "csv_path = 'diamonds.csv'\n",
    "df_pandas = pd.read_csv(dataset_url)\n",
    "df_pandas.to_csv(csv_path, index=False)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DiamondsPipeline\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"carat\", DoubleType(), True),\n",
    "    StructField(\"cut\", StringType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"clarity\", StringType(), True),\n",
    "    StructField(\"depth\", DoubleType(), True),\n",
    "    StructField(\"table\", DoubleType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"x\", DoubleType(), True),\n",
    "    StructField(\"y\", DoubleType(), True),\n",
    "    StructField(\"z\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(csv_path, header=True, schema=schema)\n",
    "df = df.cache()  # Cache para optimizar la ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline de Regresión para predecir 'price'\n",
    "\n",
    "- Utilizamos un conjunto de herramientas de preprocesamiento (como Imputer para valores faltantes, StringIndexer para convertir variables categóricas en números, y OneHotEncoder para crear variables binarias) para preparar los datos.\n",
    "\n",
    "- Escalamos algunas de las características (como carat) utilizando MinMaxScaler.\n",
    "\n",
    "- Creamos un modelo de regresión con RandomForestRegressor para predecir el precio de los diamantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regresión R2: 0.906992235373207\n",
      "Regresión RMSE: 1216.655116436566\n",
      "Regresión MAE: 685.959520354088\n"
     ]
    }
   ],
   "source": [
    "# Imputer para columnas numéricas\n",
    "imputer_reg = Imputer(\n",
    "    inputCols=[\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"],\n",
    "    outputCols=[\"carat_imputed\", \"depth_imputed\", \"table_imputed\", \"x_imputed\", \"y_imputed\", \"z_imputed\"]\n",
    ")\n",
    "\n",
    "# Indexar variables categóricas: \"cut\", \"color\", \"clarity\"\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in [\"cut\", \"color\", \"clarity\"]]\n",
    "\n",
    "# One-Hot Encoding para las variables categóricas indexadas\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_ohe\") for col in [\"cut\", \"color\", \"clarity\"]]\n",
    "\n",
    "# Escalado de la variable \"carat\" (usando la columna imputada)\n",
    "carat_assembler = VectorAssembler(inputCols=[\"carat_imputed\"], outputCol=\"carat_vector\")\n",
    "scaler = MinMaxScaler(inputCol=\"carat_vector\", outputCol=\"carat_scaled\")\n",
    "\n",
    "# VectorAssembler para combinar las features finales (usando columnas imputadas)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"carat_scaled\", \"depth_imputed\", \"table_imputed\", \"x_imputed\", \"y_imputed\", \"z_imputed\", \"cut_ohe\", \"color_ohe\", \"clarity_ohe\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Modelo de regresión\n",
    "regressor = RandomForestRegressor(featuresCol=\"features\", labelCol=\"price\")\n",
    "\n",
    "# Definición del pipeline de regresión con imputer\n",
    "reg_pipeline = Pipeline(stages=[imputer_reg] + indexers + encoders + [carat_assembler, scaler, assembler, regressor])\n",
    "\n",
    "# Entrenamiento y predicción del modelo de regresión\n",
    "reg_model = reg_pipeline.fit(df)\n",
    "df_pred_reg = reg_model.transform(df)\n",
    "\n",
    "# Evaluación del modelo de regresión\n",
    "evaluator_reg = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_reg.evaluate(df_pred_reg)\n",
    "print(f\"Regresión R2: {r2}\")\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(df_pred_reg)\n",
    "print(f\"Regresión RMSE: {rmse}\")\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator_mae.evaluate(df_pred_reg)\n",
    "print(f\"Regresión MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline de Clasificación para predecir 'cut'\n",
    "\n",
    "- Similar al pipeline de regresión, pero en lugar de predecir el precio, predecimos la categoría cut de los diamantes.\n",
    "\n",
    "- Usamos un modelo de clasificación RandomForestClassifier para este objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación Accuracy: 0.6980163144234335\n"
     ]
    }
   ],
   "source": [
    "# Indexar \"cut\" para usarlo como label\n",
    "label_indexer = StringIndexer(inputCol=\"cut\", outputCol=\"label\")\n",
    "\n",
    "# Imputer para columnas numéricas\n",
    "imputer_clf = Imputer(\n",
    "    inputCols=[\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"],\n",
    "    outputCols=[\"carat_imputed\", \"depth_imputed\", \"table_imputed\", \"x_imputed\", \"y_imputed\", \"z_imputed\"]\n",
    ")\n",
    "\n",
    "# Para clasificación, se usan las variables \"color\" y \"clarity\" como features categóricas\n",
    "indexers_clf = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in [\"color\", \"clarity\"]]\n",
    "encoders_clf = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_ohe\") for col in [\"color\", \"clarity\"]]\n",
    "\n",
    "# Reutilizamos el escalado de \"carat\" (usando la versión imputada)\n",
    "carat_assembler_clf = VectorAssembler(inputCols=[\"carat_imputed\"], outputCol=\"carat_vector\")\n",
    "scaler_clf = MinMaxScaler(inputCol=\"carat_vector\", outputCol=\"carat_scaled\")\n",
    "\n",
    "# VectorAssembler para el pipeline de clasificación (usando columnas imputadas)\n",
    "assembler_clf = VectorAssembler(\n",
    "    inputCols=[\"carat_scaled\", \"depth_imputed\", \"table_imputed\", \"x_imputed\", \"y_imputed\", \"z_imputed\", \"color_ohe\", \"clarity_ohe\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Modelo de clasificación\n",
    "classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Definición del pipeline de clasificación con imputer\n",
    "clf_pipeline = Pipeline(stages=[label_indexer, imputer_clf] + indexers_clf + encoders_clf + [carat_assembler_clf, scaler_clf, assembler_clf, classifier])\n",
    "\n",
    "# Entrenamiento y predicción del modelo de clasificación\n",
    "clf_model = clf_pipeline.fit(df)\n",
    "df_pred_clf = clf_model.transform(df)\n",
    "\n",
    "# Evaluación del modelo de clasificación\n",
    "evaluator_clf = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_clf.evaluate(df_pred_clf)\n",
    "print(f\"Clasificación Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GridSearch con CrossValidation\n",
    "\n",
    "- Se realiza un ajuste de parámetros (en este caso, el número de árboles del modelo) utilizando validación cruzada (CrossValidator).\n",
    "\n",
    "- Después de la validación cruzada, evaluamos el modelo usando el valor R2 para regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation R2: 0.906992235373207\n"
     ]
    }
   ],
   "source": [
    "paramGrid = ParamGridBuilder().addGrid(regressor.numTrees, [1, 20]).build()\n",
    "crossval = CrossValidator(\n",
    "    estimator=reg_pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator_reg,\n",
    "    numFolds=3\n",
    ")\n",
    "cv_model = crossval.fit(df)\n",
    "cv_r2 = evaluator_reg.evaluate(cv_model.transform(df))\n",
    "print(f\"Cross-Validation R2: {cv_r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
